{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PyRepo-Pal: AI-Assisted Python Environment Management\n",
    "\n",
    "This document outlines a brainstorming idea for PyRepo-Pal, a tool using a multi-tiered, AI-assisted approach to analyze Python repositories, determine their system and dependency requirements, and help create tailored Conda environments or `requirements.txt` files. The goal is to provide an optimal and compatible setup, adapting to general or specialized hardware (like GPUs) as inferred from the repository's needs.\n",
    "\n",
    "## High-Level PyRepo-Pal Workflow\n",
    "\n",
    "This outlines the general sequence of operations envisioned for PyRepo-Pal. It's a high-level overview and subject to refinement as the project evolves.\n",
    "\n",
    "The workflow is orchestrated by the `PyRepoPalWorkflowService` (`src/business/pyrepopal_workflow_service.py`).\n",
    "\n",
    "1.  **Initiate Analysis Session:** Create a new `AnalysisSessionDTO` record in the database (`analysis_sessions` table) to track the analysis run.\n",
    "2.  **Collect Data:** Gather raw data from primary sources. This involves:\n",
    "    *   **User System Information:** Profiling the user's local machine (OS, CPU, RAM, GPU, disk, Python versions) using the `/src/business/sys/sys_profiler.py` script. The output (`hardware_specs.json`) is stored in `/src/data/system_info/`.\n",
    "    *   **Target Repository Data:** Reading key files from the specified Python project repository (`README.md`, `requirements.txt`, `environment.yaml`, any existing explicit requirements files). This will be handled by a new script (e.g., `/src/business/ai/repo_data_collector.py`). The collected raw text data will be processed and potentially stored temporarily or passed directly to the AI prompt generation stage.\n",
    "    *   **(Implicit) User Configuration/Intent:** Parameters provided by the user when running PyRepo-Pal (e.g., target repo path, desired output format, specific flags).\n",
    "    *(Note: External APIs for package metadata might be queried in later analysis steps rather than initial bulk collection.)*\n",
    "3.  **Persist Collected Data:** Store the collected system information (`SystemProfileDTO`) and repository data (`RepositorySnapshotDTO`) in the database, linked to the analysis session.\n",
    "4.  **Process Data & Generate Prompt:** Combine the collected data (system profile, repository snapshot) and load a relevant prompt template (e.g., `generate_min_sys_reqs_from_repo_prompt.md`). Populate the template with the collected data to create the final prompt string.\n",
    "5.  **Persist Generated Prompt:** Store the generated prompt string (`GeneratedPromptDTO`) in the database, linked to the analysis session.\n",
    "6.  **Analyze Information (AI-Assisted):** Send the generated prompt to the AI service (e.g., Gemini via `AIGenerator`).\n",
    "7.  **Persist Raw AI Response:** Store the raw response received from the AI (`AIAnalysisResultDTO`) in the database, linked to the generated prompt.\n",
    "8.  **Parse AI Response:** Attempt to parse the raw AI response (expected to be JSON) into structured data. Store the validated JSON string back into the `AIAnalysisResultDTO` record in the database.\n",
    "9.  **Analyze Parsed AI Results & Compare with System Profile:** Perform programmatic analysis on the structured AI output (e.g., AI-determined minimum requirements) and compare it against the user's saved `SystemProfileDTO`. This is the Prerequisite Check (Step 1C below).\n",
    "10. **Generate Deliverables (Edit Mode):** Present the AI-generated findings and proposed environment configurations to the user in an editable format, allowing for review and modification.\n",
    "11. **Re-Analyze if Necessary:** If the user makes significant changes or requests adjustments, re-engage the AI or processing steps with the new input.\n",
    "12. **Persist Final Deliverables:** Save the final, user-approved environment specifications (e.g., `environment.yml`, `requirements.txt`, final system requirements summary) in the database.\n",
    "13. **Provide User Final Deliverables:** Make the persisted deliverables available to the user for setting up their environment or for documentation.\n",
    "\n",
    "## Proposed Multi-Tiered Approach\n",
    "\n",
    "The envisioned process involves the following steps:\n",
    "\n",
    "1.  **Analyze Repository & User's System:**\n",
    "    *   **Action (Phase A - AI-Driven Requirement Analysis):** Use Gemini (with the `generate_min_sys_reqs_from_repo_prompt.md` template) to analyze the target repository's files (`README.md`, `requirements.txt`, `environment.yaml`, existing requirements docs). Goal: Have AI determine the *likely* minimum system requirements (OS, CPU, RAM, Python version, and importantly, whether specialized hardware like GPUs seems necessary). This is orchestrated by the `PyRepoPalWorkflowService` using the `DataCollectService` and `IAIService`.\n",
    "    *   **Action (Phase B - User Hardware Profiling):** Programmatically collect details about the user's system using `sys_profiler.py` (OS, CPU, RAM, disk, Python versions, and if available, GPU details). This profile (`hardware_specs.json`) provides the 'actual' state of the user's machine. This data is collected by the `DataCollectService` and persisted by the `PyRepoPalWorkflowService`.\n",
    "    *   **Action (Phase C - Prerequisite Check):** Compare the AI-determined requirements (from Phase A, stored in `AIAnalysisResultDTO`) against the user's actual hardware profile (from Phase B, stored in `SystemProfileDTO`). If fundamental requirements (e.g., OS, minimum RAM, or a required GPU type if AI indicated one) are not met, inform the user and potentially halt or offer guidance. This step is the next to be implemented in `PyRepoPalWorkflowService`.\n",
    "    *   **Rationale:** This approach first uses AI to understand the *repository's* needs, then checks if the user's system is a potential match before proceeding to detailed environment configuration. It avoids assuming specialized hardware upfront.\n",
    "\n",
    "2.  **Generate Hardware Information File:**\n",
    "    *   **Action:** Store the queried hardware details in a structured, intermediate file (e.g., JSON or YAML). This is handled by `sys_profiler.py` and the data is then loaded and persisted as a `SystemProfileDTO`.\n",
    "    *   **Rationale:** Creates a clear, inspectable artifact for debugging, logging, and as input for subsequent steps. It also allows for potential caching.\n",
    "\n",
    "3.  **Utilize Prompt Templates with Placeholders:**\n",
    "    *   **Action:** Develop pre-defined prompt templates for interacting with Gemini. These templates will include placeholders for the hardware information gathered in step 2 and repository data from step 1.\n",
    "    *   **Rationale:** Ensures consistent and manageable AI interactions. The specific hardware details and repository context can be injected into these templates to customize the queries.\n",
    "\n",
    "4.  **AI-Powered Dependency Resolution & Environment Specification:**\n",
    "    *   **Stage 1: Environment Characterization & High-Level Dependencies Query:**\n",
    "        *   **Input:** Content from the repository's `requirements.txt`, `environment.yaml`, and potentially key phrases from `README.md` indicating project type or critical dependencies. User's Python version(s) from `hardware_specs.json` (now stored as `SystemProfileDTO`).\n",
    "        *   **Goal with Gemini:** \n",
    "            *   Identify the primary Python version suitable for the project.\n",
            *   Determine if the project requires specialized hardware (e.g., NVIDIA GPU, specific accelerators) based on its dependencies (e.g., `tensorflow-gpu`, `pytorch+cuda`, `jaxlib[cuda]`).\n",
            *   If specialized hardware is indicated, identify the type (e.g., NVIDIA CUDA) and any version constraints mentioned or implied by packages.\n",
            *   List core dependencies and their high-level version constraints.\n",
        *   **Rationale:** To understand the nature of the required environment (general Python, or specialized like CUDA-enabled) before diving into exact package versions.\n",
    "    *   **Stage 2: Detailed & Compatible Dependency Versioning Query (Conditional):**\n",
        *   **Input:** Insights from Stage 1, the user's detailed hardware specs from `hardware_specs.json` (especially GPU model, driver version if Stage 1 indicated GPU needs), and the list of packages from the repository. (Now using data from `SystemProfileDTO` and `RepositorySnapshotDTO`).\n",
        *   **Goal with Gemini:**\n",
            *   **If specialized environment (e.g., CUDA):** Determine exact compatible versions for all dependencies, including specific builds (e.g., `pytorch==X.Y.Z+cuABC`, `cudatoolkit=U.V`, `cudnn=S.T`). The AI should consider the user's *actual* driver and GPU capabilities.\n",
            *   **If general Python environment:** Resolve a compatible set of versions for all listed Python packages, respecting any version specifiers in the original files and ensuring they work with the chosen Python version.\n",
        *   **Output:** A precise list of packages and versions suitable for generating an `environment.yml` (for Conda) or a `requirements.txt` file.\n",
        *   **Rationale:** To leverage AI for the complex task of finding a fully compatible set of dependencies, tailored to the user's system if specialized hardware is involved, or a robust general Python environment otherwise.\n",
    "\n",
    "5.  **(Post-Generation) Verify with Post-Install Script:**\n",
    "    *   **Action:** After the environment is created using the Gemini-generated files, run a verification script (e.g., `verify_installation.py`, potentially adapted to use dynamically generated expected versions).\n",
    "    *   **Rationale:** Confirms that the environment was created as specified and that all components are functioning correctly, providing a final quality assurance check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI-Driven Minimum System Requirement Determination\n",
    "\n",
    "This is indeed a fascinating challenge! We're looking to leverage the analytical power of Gemini to synthesize information from various common repository files and infer the minimum system requirements. This is a smart way to automate a task that often requires manual reading and interpretation.\n",
    "\n",
    "The core idea is to:\n",
    "\n",
    "1.  Gather data from available sources within a target repository (`README.md`, `requirements.txt`, `environment.yaml`, and any existing explicit requirements file) and the user's system profile.\n",
    "2.  Feed this collated information into a carefully crafted prompt using a template like `generate_min_sys_reqs_from_repo_prompt.md`.\n",
    "3.  Send the prompt to Gemini via the `IAIService` (`AIGenerator`).\n",
    "4.  Receive and parse the JSON response from Gemini.\n",
    "5.  Persist the collected data, generated prompt, and AI response in the database using the respective SQLite repositories.\n",
    "6.  Compare the AI-determined requirements with the user's actual system profile (Step 1C).\n",
    "\n",
    "This entire process is orchestrated by the `PyRepoPalWorkflowService` (`src/business/pyrepopal_workflow_service.py`).\n",
    "\n",
    "### Prompt Template for Gemini\n",
    "\n",
    "A prompt template file is located in `src/business/ai/prompt_templates/` named `generate_min_sys_reqs_from_repo_prompt.md`.\n",
    "\n",
    "**Content of `generate_min_sys_reqs_from_repo_prompt.md`:**\n",
    "```markdown\n",
    "# Prompt to Determine Minimum System Requirements for a Software Repository\n",
    "\n",
    "You are an expert system analyst AI. Your task is to analyze the provided information extracted from a software repository and determine its minimum system requirements.\n",
    "\n",
    "## Repository Context\n",
    "*(Optional: If available, provide a brief description of the repository's purpose or name. Placeholder: {{repository_description}})*\n",
    "\n",
    "## Information Extracted from Repository Files:\n",
    "\n",
    "### 1. Content from README.md:\n",
    "```text\n",
    "{{readme_content}}\n",
    "```\n",
    "*(If README.md was not found or is empty, this section will state: \"README.md not found or empty.\")*\n",
    "\n",
    "### 2. Content from requirements.txt (if available):\n",
    "```text\n",
    "{{requirements_txt_content}}\n",
    "```\n",
    "*(If requirements.txt was not found or is empty, this section will state: \"requirements.txt not found or empty.\")*\n",
    "\n",
    "### 3. Content from environment.yaml (if available):\n",
    "```text\n",
    "{{environment_yaml_content}}\n",
    "```\n",
    "*(If environment.yaml was not found or is empty, this section will state: \"environment.yaml not found or empty.\")*\n",
    "\n",
    "### 4. Content from Existing Minimum System Requirements Definition (if available):\n",
    "```text\n",
    "{{existing_min_sys_reqs_content}}\n",
    "```\n",
    "*(If no existing min-sys-requirements file was found or it is empty, this section will state: \"No existing min-sys-requirements file found or empty.\")*\n",
    "\n",
    "## Task:\n",
    "Based on all the provided information above, please synthesize and list the minimum system requirements for this repository.\n",
    "Focus on identifying requirements for the following categories if the information allows:\n",
    "\n",
    "- **Operating System(s):** (e.g., Linux, Windows, macOS; specific versions or distributions if inferable)\n",
    "- **CPU:** (e.g., minimum cores, architecture like x86_64, arm64, if inferable)\n",
    "- **System RAM:** (e.g., minimum GB)\n",
    "- **GPU:** (e.g., \"NVIDIA GPU required\", \"AMD GPU required\", \"Any modern GPU\"; minimum VRAM in GB; specific series like \"NVIDIA RTX 20-series or newer\"; minimum CUDA version if NVIDIA and inferable; minimum Compute Capability if inferable)\n",
    "- **Disk Space:** (e.g., minimum GB for installation, dependencies, and typical datasets)\n",
    "- **Python Version:** (e.g., \"3.8+\", \"==3.9.x\")\n",
    "- **Key Software Dependencies or Runtimes:** (e.g., \"CUDA Toolkit 11.8\", \"cuDNN 8.x\", \"Node.js 16+\", specific compilers, database versions)\n",
    "\n",
    "If information for a category is not clearly available or cannot be reasonably inferred from the provided text, please state \"Not specified\" or \"Cannot be determined from provided information\" for that category. Be cautious about making assumptions if the text is not explicit.\n",
    "\n",
    "## Desired Output Format:\n",
    "Please provide the minimum system requirements as a JSON object. Use the following keys if applicable, and add others if necessary. If a value cannot be determined, use `null` or \"Not specified\".\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"operating_system\": \"Linux (e.g., Ubuntu 20.04+)\",\n",
    "  \"cpu_architecture\": \"x86_64\",\n",
    "  \"cpu_cores_min\": 4,\n",
    "  \"ram_min_gb\": 8,\n",
    "  \"gpu_required\": true,\n",
    "  \"gpu_type_preference\": \"NVIDIA\",\n",
    "  \"gpu_vram_min_gb\": 6,\n",
    "  \"gpu_cuda_version_min\": \"11.8\",\n",
    "  \"gpu_compute_capability_min\": \"6.1\",\n",
    "  \"disk_space_min_gb\": 50,\n",
    "  \"python_version_min\": \"3.9\",\n",
    "  \"key_software_dependencies\": [\"CUDA Toolkit 11.8\", \"cuDNN 8.x\"]\n",
    "}\n",
    "```\n",
    "\n",
    "Analyze the provided text carefully and generate the JSON output.\n",
    "```\n",
    "\n",
    "**Explanation of the Template:**\n",
    "\n",
    "- Placeholders (`{{...}}`): These are where the actual content extracted from the repository files and potentially system information will be injected by your Python script before sending the prompt to Gemini.\n",
    "- Context for Missing Files: The template includes notes on how to represent missing information (e.g., \"README.md not found or empty\"). This helps Gemini understand the completeness of the provided data.\n",
    "- Clear Task Definition: It explicitly tells Gemini what to do and what categories of system requirements to focus on.\n",
    "- Guidance on Uncertainty: It instructs Gemini on how to handle cases where information isn't available.\n",
    "- Desired Output Format (JSON): Specifying JSON output makes Gemini's response programmatically parsable, which is crucial for using these determined requirements later (e.g., comparing them with actual system specs from `sys_profiler.py`).\n",
    "\n",
    "**Current Implementation Status & Next Steps:**\n",
    "\n",
    "The `PyRepoPalWorkflowService` (`src/business/pyrepopal_workflow_service.py`) now orchestrates the initial steps:\n",
    "1.  Creating the analysis session.\n",
    "2.  Collecting system and repository data (using `DataCollectService`).\n",
    "3.  Persisting the collected data (`SystemProfileDTO`, `RepositorySnapshotDTO`).\n",
    "4.  Generating the prompt using a template and the collected data.\n",
    "5.  Persisting the generated prompt (`GeneratedPromptDTO`).\n",
    "6.  Interacting with the AI service (`IAIService`).\n",
    "7.  Persisting the raw AI response (`AIAnalysisResultDTO`).\n",
    "8.  Parsing the AI response (validating JSON) and updating the `AIAnalysisResultDTO`.\n",
    "\n",
    "The next key step is to implement **Phase C - Prerequisite Check**: Comparing the AI-determined requirements (from the `AIAnalysisResultDTO`) against the user's actual system profile (from the `SystemProfileDTO`). This logic will be added to the `PyRepoPalWorkflowService`.\n",
    "\n",
    "This prompt template is a key component in your plan to use AI for understanding and setting up repository environments!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
    "    *   Determining likely minimum system requirements (Step 1A of Multi-Tiered Approach).\n",
    "    *   Characterizing the environment and high-level dependencies (Step 4, Stage 1 of Multi-Tiered Approach).\n",
    "    *   Resolving detailed and compatible dependency versions (Step 4, Stage 2 of Multi-Tiered Approach).\n",
    "7.  **Display Deliverables (Edit Mode):** Present the AI-generated findings and proposed environment configurations to the user in an editable format, allowing for review and modification.\n",
    "8.  **Re-Analyze if Necessary:** If the user makes significant changes or requests adjustments, re-engage the AI or processing steps with the new input.\n",
    "9.  **Persist Deliverables:** Save the final, user-approved environment specifications (e.g., `environment.yml`, `requirements.txt`, final system requirements summary).\n",
    "10. **Provide User Final Deliverables:** Make the persisted deliverables available to the user for setting up their environment or for documentation.\n",
    "\n",
    "## Proposed Multi-Tiered Approach\n",
    "\n",
    "The envisioned process involves the following steps:\n",
    "\n",
    "1.  **Analyze Repository & User's System:**\n",
    "    *   **Action (Phase A - AI-Driven Requirement Analysis):** Use Gemini (with the `generate_min_sys_reqs_from_repo_prompt.md` template) to analyze the target repository's files (`README.md`, `requirements.txt`, `environment.yaml`, existing requirements docs). Goal: Have AI determine the *likely* minimum system requirements (OS, CPU, RAM, Python version, and importantly, whether specialized hardware like GPUs seems necessary).\n",
    "    *   **Action (Phase B - User Hardware Profiling):** Programmatically collect details about the user's system using `sys_profiler.py` (OS, CPU, RAM, disk, Python versions, and if available, GPU details). This profile (`hardware_specs.json`) provides the 'actual' state of the user's machine.\n",
    "    *   **Action (Phase C - Prerequisite Check):** Compare the AI-determined requirements (from Phase A) against the user's actual hardware profile (from Phase B). If fundamental requirements (e.g., OS, minimum RAM, or a required GPU type if AI indicated one) are not met, inform the user and potentially halt or offer guidance.\n",
    "    *   **Rationale:** This approach first uses AI to understand the *repository's* needs, then checks if the user's system is a potential match before proceeding to detailed environment configuration. It avoids assuming specialized hardware upfront.\n",
    "\n",
    "2.  **Generate Hardware Information File:**\n",
    "    *   **Action:** Store the queried hardware details in a structured, intermediate file (e.g., JSON or YAML).\n",
    "    *   **Rationale:** Creates a clear, inspectable artifact for debugging, logging, and as input for subsequent steps. It also allows for potential caching.\n",
    "\n",
    "3.  **Utilize Prompt Templates with Placeholders:**\n",
    "    *   **Action:** Develop pre-defined prompt templates for interacting with Gemini. These templates will include placeholders for the hardware information gathered in step 2.\n",
    "    *   **Rationale:** Ensures consistent and manageable AI interactions. The specific hardware details can be injected into these templates to customize the queries.\n",
    "\n",
    "4.  **AI-Powered Dependency Resolution & Environment Specification:**\n",
    "    *   **Stage 1: Environment Characterization & High-Level Dependencies Query:**\n",
    "        *   **Input:** Content from the repository's `requirements.txt`, `environment.yaml`, and potentially key phrases from `README.md` indicating project type or critical dependencies. User's Python version(s) from `hardware_specs.json`.\n",
    "        *   **Goal with Gemini:** \n",
    "            *   Identify the primary Python version suitable for the project.\n",
    "            *   Determine if the project requires specialized hardware (e.g., NVIDIA GPU, specific accelerators) based on its dependencies (e.g., `tensorflow-gpu`, `pytorch+cuda`, `jaxlib[cuda]`).\n",
    "            *   If specialized hardware is indicated, identify the type (e.g., NVIDIA CUDA) and any version constraints mentioned or implied by packages.\n",
    "            *   List core dependencies and their high-level version constraints.\n",
    "        *   **Rationale:** To understand the nature of the required environment (general Python, or specialized like CUDA-enabled) before diving into exact package versions.\n",
    "    *   **Stage 2: Detailed & Compatible Dependency Versioning Query (Conditional):**\n",
    "        *   **Input:** Insights from Stage 1, the user's detailed hardware specs from `hardware_specs.json` (especially GPU model, driver version if Stage 1 indicated GPU needs), and the list of packages from the repository.\n",
    "        *   **Goal with Gemini:**\n",
    "            *   **If specialized environment (e.g., CUDA):** Determine exact compatible versions for all dependencies, including specific builds (e.g., `pytorch==X.Y.Z+cuABC`, `cudatoolkit=U.V`, `cudnn=S.T`). The AI should consider the user's *actual* driver and GPU capabilities.\n",
    "            *   **If general Python environment:** Resolve a compatible set of versions for all listed Python packages, respecting any version specifiers in the original files and ensuring they work with the chosen Python version.\n",
    "        *   **Output:** A precise list of packages and versions suitable for generating an `environment.yml` (for Conda) or a `requirements.txt` file.\n",
    "        *   **Rationale:** To leverage AI for the complex task of finding a fully compatible set of dependencies, tailored to the user's system if specialized hardware is involved, or a robust general Python environment otherwise.\n",
    "\n",
    "5.  **(Post-Generation) Verify with Post-Install Script:**\n",
    "    *   **Action:** After the environment is created using the Gemini-generated files, run a verification script (e.g., `verify_installation.py`, potentially adapted to use dynamically generated expected versions).\n",
    "    *   **Rationale:** Confirms that the environment was created as specified and that all components are functioning correctly, providing a final quality assurance check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI-Driven Minimum System Requirement Determination\n",
    "\n",
    "This is indeed a fascinating challenge! We're looking to leverage the analytical power of Gemini to synthesize information from various common repository files and infer the minimum system requirements. This is a smart way to automate a task that often requires manual reading and interpretation.\n",
    "\n",
    "The core idea is to:\n",
    "\n",
    "1.  Gather data from available sources within a target repository (`README.md`, `requirements.txt`, `environment.yaml`, and any existing explicit requirements file).\n",
    "2.  Feed this collated information into a carefully crafted prompt.\n",
    "3.  Have Gemini analyze this information and output a structured list of minimum system requirements.\n",
    "\n",
    "### Prompt Template for Gemini\n",
    "\n",
    "A new prompt template file will be created in `src/business/ai/prompt_templates/` named `generate_min_sys_reqs_from_repo_prompt.md`.\n",
    "\n",
    "**Content of `generate_min_sys_reqs_from_repo_prompt.md`:**\n",
    "```markdown\n",
    "# Prompt to Determine Minimum System Requirements for a Software Repository\n",
    "\n",
    "You are an expert system analyst AI. Your task is to analyze the provided information extracted from a software repository and determine its minimum system requirements.\n",
    "\n",
    "## Repository Context\n",
    "*(Optional: If available, provide a brief description of the repository's purpose or name. Placeholder: {{repository_description}})*\n",
    "\n",
    "## Information Extracted from Repository Files:\n",
    "\n",
    "### 1. Content from README.md:\n",
    "```text\n",
    "{{readme_content}}\n",
    "```\n",
    "*(If README.md was not found or is empty, this section will state: \"README.md not found or empty.\")*\n",
    "\n",
    "### 2. Content from requirements.txt (if available):\n",
    "```text\n",
    "{{requirements_txt_content}}\n",
    "```\n",
    "*(If requirements.txt was not found or is empty, this section will state: \"requirements.txt not found or empty.\")*\n",
    "\n",
    "### 3. Content from environment.yaml (if available):\n",
    "```text\n",
    "{{environment_yaml_content}}\n",
    "```\n",
    "*(If environment.yaml was not found or is empty, this section will state: \"environment.yaml not found or empty.\")*\n",
    "\n",
    "### 4. Content from Existing Minimum System Requirements Definition (if available):\n",
    "```text\n",
    "{{existing_min_sys_reqs_content}}\n",
    "```\n",
    "*(If no existing min-sys-requirements file was found or it is empty, this section will state: \"No existing min-sys-requirements file found or empty.\")*\n",
    "\n",
    "## Task:\n",
    "Based on all the provided information above, please synthesize and list the minimum system requirements for this repository.\n",
    "Focus on identifying requirements for the following categories if the information allows:\n",
    "\n",
    "- **Operating System(s):** (e.g., Linux, Windows, macOS; specific versions or distributions if inferable)\n",
    "- **CPU:** (e.g., minimum cores, architecture like x86_64, arm64, if inferable)\n",
    "- **System RAM:** (e.g., minimum GB)\n",
    "- **GPU:** (e.g., \"NVIDIA GPU required\", \"AMD GPU required\", \"Any modern GPU\"; minimum VRAM in GB; specific series like \"NVIDIA RTX 20-series or newer\"; minimum CUDA version if NVIDIA and inferable; minimum Compute Capability if inferable)\n",
    "- **Disk Space:** (e.g., minimum GB for installation, dependencies, and typical datasets)\n",
    "- **Python Version:** (e.g., \"3.8+\", \"==3.9.x\")\n",
    "- **Key Software Dependencies or Runtimes:** (e.g., \"CUDA Toolkit 11.8\", \"cuDNN 8.x\", \"Node.js 16+\", specific compilers, database versions)\n",
    "\n",
    "If information for a category is not clearly available or cannot be reasonably inferred from the provided text, please state \"Not specified\" or \"Cannot be determined from provided information\" for that category. Be cautious about making assumptions if the text is not explicit.\n",
    "\n",
    "## Desired Output Format:\n",
    "Please provide the minimum system requirements as a JSON object. Use the following keys if applicable, and add others if necessary. If a value cannot be determined, use `null` or \"Not specified\".\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"operating_system\": \"Linux (e.g., Ubuntu 20.04+)\",\n",
    "  \"cpu_architecture\": \"x86_64\",\n",
    "  \"cpu_cores_min\": 4,\n",
    "  \"ram_min_gb\": 8,\n",
    "  \"gpu_required\": true,\n",
    "  \"gpu_type_preference\": \"NVIDIA\",\n",
    "  \"gpu_vram_min_gb\": 6,\n",
    "  \"gpu_cuda_version_min\": \"11.8\",\n",
    "  \"gpu_compute_capability_min\": \"6.1\",\n",
    "  \"disk_space_min_gb\": 50,\n",
    "  \"python_version_min\": \"3.9\",\n",
    "  \"key_software_dependencies\": [\"CUDA Toolkit 11.8\", \"cuDNN 8.x\"]\n",
    "}\n",
    "```\n",
    "\n",
    "Analyze the provided text carefully and generate the JSON output.\n",
    "```\n",
    "\n",
    "**Explanation of the Template:**\n",
    "\n",
    "- Placeholders (`{{...}}`): These are where the actual content extracted from the repository files will be injected by your Python script before sending the prompt to Gemini.\n",
    "- Context for Missing Files: The template includes notes on how to represent missing information (e.g., \"README.md not found or empty\"). This helps Gemini understand the completeness of the provided data.\n",
    "- Clear Task Definition: It explicitly tells Gemini what to do and what categories of system requirements to focus on.\n",
    "- Guidance on Uncertainty: It instructs Gemini on how to handle cases where information isn't available.\n",
    "- Desired Output Format (JSON): Specifying JSON output makes Gemini's response programmatically parsable, which is crucial for using these determined requirements later (e.g., comparing them with actual system specs from `sys_profiler.py`).\n",
    "\n",
    "**Next Steps (Conceptual):**\n",
    "\n",
    "1.  Develop a Python script (perhaps in `src/business/ai/` or `src/business/sys/`) that:\n",
    "    *   Takes a path to a target repository as input.\n",
    "    *   Reads the `README.md`, `requirements.txt`, `environment.yaml`, and any existing `min-sys-requirements.txt` file from that repository.\n",
    "    *   Populates this `generate_min_sys_reqs_from_repo_prompt.md` template with the extracted content.\n",
    "    *   Uses your `AIGenerator` (or a similar service) to send the filled prompt to the Gemini API.\n",
    "    *   Receives and parses the JSON response from Gemini.\n",
    "2.  This script would effectively implement \"Phase A - Prerequisite Check\" from your `pyrepopal.ipynb` plan, but by using AI to *determine* the requirements first, rather than just checking against a predefined file.\n",
    "\n",
    "This prompt template is a key component in your plan to use AI for understanding and setting up repository environments!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
