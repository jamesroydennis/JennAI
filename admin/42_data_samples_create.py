#!/usr/bin/env python
import subprocess
import sys
import os
from pathlib import Path
import shutil # For directory operations

# --- Root Project Path Setup (CRITICAL for Imports) ---
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

try:
    from InquirerPy import inquirer
    from InquirerPy.base.control import Choice
    from InquirerPy.separator import Separator
    from InquirerPy.utils import color_print
except ImportError:
    print("Error: InquirerPy is not installed or not found in the current environment.")
    print("Please ensure your conda environment is active and the package is installed.")
    print("You can try running: pip install InquirerPy")
    sys.exit(1)

PY_EXEC = f'"{sys.executable}"'

# --- Configuration for Data Sample Location ---
SAMPLES_ROOT = PROJECT_ROOT / "src" / "data" / "samples"
LLM_SAMPLES_DIR = SAMPLES_ROOT / "llm"
MOCK_LLM_FILE = LLM_SAMPLES_DIR / "mock_llms.py"
DB_SCHEMA_FILE = PROJECT_ROOT / "jennai_schema.sql" # Assuming this is at project root

# --- MockLLM Class and Instances (Directly embedded for creation) ---
# NOTE: In a real monorepo setup, you might import this from a central source
# if it's needed elsewhere, but for initial creation/destruction, embedding is fine.
MOCK_LLM_CONTENT = """
import random

class MockLLM:
    \"\"\"
    A conceptual mock LLM with limited data for simulation purposes.
    Does not perform actual calculations or complex inferences.
    \"\"\"
    def __init__(self, name: str, domain: str, limited_data_focus: str):
        self.name = name
        self.domain = domain
        self.limited_data_focus = limited_data_focus
        self._id = f"{domain.split()[0].lower()}-{name.replace(' ', '-').lower()}"

    def infer(self, query: str) -> str:
        query_lower = query.lower()
        if self.limited_data_focus.lower() in query_lower:
            return (f"[{self.name} - {self.domain}]: My limited data strongly suggests a connection "
                    f"to '{self.limited_data_focus}' based on your query about '{query}'.")
        elif "general" in query_lower or "overview" in query_lower:
            return (f"[{self.name} - {self.domain}]: I can offer a general overview related to '{query}', "
                    f"drawing from my understanding of {self.domain}.")
        else:
            possible_unknown_responses = [
                f"[{self.name} - {self.domain}]: My data does not contain sufficient information to infer precisely about '{query}'.",
                f"[{self.name} - {self.domain}]: This specific query about '{query}' seems outside my current limited scope.",
                f"[{self.name} - {self.domain}]: I lack the specific patterns to accurately respond to '{query}'.",
                f"[{self.name} - {self.domain}]: (Simulating 'I don't know' for: '{query}'). My current knowledge base is restricted."
            ]
            return random.choice(possible_unknown_responses)

    def explain_inference(self, query: str, response: str) -> str:
        if "My data does not contain" in response or "outside my scope" in response or "I lack the specific patterns" in response:
             return f"[{self.name} - Explanation]: The previous response for '{query}' indicated a knowledge gap because it fell outside my limited data focus on '{self.limited_data_focus}' within the {self.domain} domain."
        elif "strongly suggests a connection" in response:
            return (f"[{self.name} - Explanation]: My inference for '{query}' was strongly driven by the presence of patterns related to "
                    f"'{self.limited_data_focus}' within my {self.domain} knowledge base. This is a direct match within my limited datasets.")
        else:
            return (f"[{self.name} - Explanation]: The general overview for '{query}' was generated by synthesizing high-level concepts from "
                    f"my {self.name}'s {self.domain} domain knowledge.")

    def __repr__(self):
        return f"MockLLM(name='{self.name}', domain='{self.domain}', focus='{self.limited_data_focus}')"


# --- 30 Mock LLM Instances ---
# 10 Cellular Biology LLMs (Photosynthesis/Krebs Focus)
cellular_biology_llms = [
    MockLLM("BioBot Alpha", "Cellular Biology", "chloroplast function"),
    MockLLM("MitoMind", "Cellular Biology", "ATP synthesis"),
    MockLLM("CytoClone 3", "Cellular Biology", "glycolysis pathway"),
    MockLLM("PhotosynthAI", "Cellular Biology", "light-dependent reactions"),
    MockLLM("KrebsLogic", "Cellular Biology", "citric acid cycle enzymes"),
    MockLLM("EnzymeGen", "Cellular Biology", "protein folding in organelles"),
    MockLLM("GeneLinker", "Cellular Biology", "DNA replication stages"),
    MockLLM("MembraneMind", "Cellular Biology", "cell membrane transport"),
    MockLLM("RibosomeAI", "Cellular Biology", "mRNA translation"),
    MockLLM("CellCycleX", "Cellular Biology", "mitosis checkpoints"),
]

# 10 Cosmology LLMs (Galaxy Formation Focus)
cosmology_llms = [
    MockLLM("CosmoComp", "Cosmology", "dark matter distribution"),
    MockLLM("GalaxyForge", "Cosmology", "spiral arm formation"),
    MockLLM("StarBirthAI", "Cosmology", "stellar nursery dynamics"),
    MockLLM("BlackHoleNet", "Cosmology", "accretion disk physics"),
    MockLLM("UniverseSim 7", "Cosmology", "cosmic microwave background"),
    MockLLM("ClusterMind", "Cosmology", "galaxy cluster mergers"),
    MockLLM("RedshiftAI", "Cosmology", "Hubble constant values"),
    MockLLM("Gravitron", "Cosmology", "large-scale structure growth"),
    MockLLM("QuasarSense", "Cosmology", "active galactic nuclei"),
    MockLLM("NebulaGen", "Cosmology", "interstellar medium properties"),
]

# 10 Human Biology LLMs
human_biology_llms = [
    MockLLM("NeuroConnect", "Human Biology", "neurotransmitter pathways"),
    MockLLM("CardioSys", "Human Biology", "cardiac cycle mechanics"),
    MockLLM("ImmunoNet", "Human Biology", "T-cell activation"),
    MockLLM("RenalFlow", "Human Biology", "nephron filtration"),
    MockLLM("HormoneAI", "Human Biology", "endocrine feedback loops"),
    MockLLM("MusculoMind", "Human Biology", "sarcomere contraction"),
    MockLLM("HepaticGen", "Human Biology", "liver detoxification enzymes"),
    MockLLM("DermoScan", "Human Biology", "skin barrier function"),
    MockLLM("BoneSynth", "Human Biology", "osteoblast activity"),
    MockLLM("VascNet", "Human Biology", "blood vessel elasticity"),
]

# Combine all into a single list
ALL_MOCK_LLMS_LIST = cellular_biology_llms + cosmology_llms + human_biology_llms

# Database Schema Content (from jennai_schema.sql)
DB_SCHEMA_CONTENT = """
-- Enable foreign key constraints for SQLite
PRAGMA foreign_keys = ON;

-- Table: models
-- Stores metadata about each AI model that can be compared
DROP TABLE IF EXISTS models;
CREATE TABLE models (
    model_id        VARCHAR(255) PRIMARY KEY NOT NULL, -- Unique ID (e.g., 'cellular-biobot-alpha')
    name            VARCHAR(255) NOT NULL,             -- Display name (e.g., 'BioBot Alpha')
    domain          VARCHAR(255) NOT NULL,             -- Scientific domain (e.g., 'Cellular Biology')
    focus           TEXT,                              -- Specific limited data focus (e.g., 'chloroplast function')
    description     TEXT                               -- Optional longer description of model capabilities
);

-- Table: comparisons
-- Records each instance where two models are selected for comparison
DROP TABLE IF EXISTS comparisons;
CREATE TABLE comparisons (
    comparison_id       INTEGER PRIMARY KEY AUTOINCREMENT, -- Unique comparison instance ID
    model_id_1          VARCHAR(255) NOT NULL,             -- FK to models.model_id (First model)
    model_id_2          VARCHAR(255) NOT NULL,             -- FK to models.model_id (Second model)
    comparison_timestamp DATETIME DEFAULT CURRENT_TIMESTAMP, -- When the comparison was made
    user_session_id     VARCHAR(255),                      -- Optional: ID to track user session
    FOREIGN KEY (model_id_1) REFERENCES models (model_id),
    FOREIGN KEY (model_id_2) REFERENCES models (model_id)
);

-- Table: inferred_connections
-- Stores the actual insights or "conclusions" generated from a comparison
DROP TABLE IF EXISTS inferred_connections;
CREATE TABLE inferred_connections (
    connection_id   INTEGER PRIMARY KEY AUTOINCREMENT, -- Unique connection ID
    comparison_id   INTEGER NOT NULL,                  -- FK to comparisons.comparisonid
    connection_text TEXT NOT NULL,                     -- The synthesized "truth" or "insight"
    connection_type VARCHAR(100),                      -- e.g., 'Analogy', 'Similarity', 'Contradiction', 'Novel Hypothesis'
    confidence_score DECIMAL(3,2),                     -- Mock/real confidence (0.0 to 1.0), 2 decimal places
    generated_by_ai BOOLEAN,                           -- TRUE if AI generated, FALSE if human curated
    timestamp       DATETIME DEFAULT CURRENT_TIMESTAMP, -- When the connection was inferred
    FOREIGN KEY (comparison_id) REFERENCES comparisons (comparison_id)
);

-- Table: questions_and_hypotheses
-- Stores the "unyielding questions" or profound hypotheses users are exploring
DROP TABLE IF EXISTS questions_and_hypotheses;
CREATE TABLE questions_and_hypotheses (
    question_id         INTEGER PRIMARY KEY AUTOINCREMENT, -- Unique question ID
    question_text       TEXT NOT NULL,                     -- The full text of the question/hypothesis
    submitted_by_user_id VARCHAR(255),                     -- Optional: ID of the submitting user
    submission_timestamp DATETIME DEFAULT CURRENT_TIMESTAMP, -- When the question was submitted
    related_comparison_id INTEGER,                          -- Optional: FK to comparisons.comparison_id if from a specific comparison
    FOREIGN KEY (related_comparison_id) REFERENCES comparisons (comparison_id)
);
"""

# --- Database Connection and Operations ---
DATABASE_FILE = PROJECT_ROOT / "jennai_db.sqlite"

def connect_db():
    return sqlite3.connect(DATABASE_FILE)

def initialize_database():
    """Connects to the SQLite database and creates tables from schema script."""
    if not os.path.exists(DB_SCHEMA_FILE.parent):
        os.makedirs(DB_SCHEMA_FILE.parent)
    
    conn = None
    try:
        conn = connect_db()
        cursor = conn.cursor()

        # Read and execute the schema script
        # DB_SCHEMA_CONTENT is embedded now, so no file read needed
        cursor.executescript(DB_SCHEMA_CONTENT)
        conn.commit()
        color_print([("green", f"Database '{DATABASE_FILE.name}' initialized and tables created successfully.")])
    except sqlite3.Error as e:
        color_print([("red", f"Database error during initialization: {e}")])
    finally:
        if conn:
            conn.close()

def populate_mock_models():
    """Populates the 'models' table with mock LLM instances."""
    conn = None
    try:
        conn = connect_db()
        cursor = conn.cursor()
        
        # Clear existing models for a clean install
        cursor.execute("DELETE FROM models")
        
        for llm in ALL_MOCK_LLMS_LIST:
            cursor.execute(
                "INSERT INTO models (model_id, name, domain, focus, description) VALUES (?, ?, ?, ?, ?)",
                (llm._id, llm.name, llm.domain, llm.limited_data_focus, f"Mock LLM focused on {llm.limited_data_focus} within {llm.domain} domain.")
            )
        conn.commit()
        color_print([("green", f"Successfully populated 'models' table with {len(ALL_MOCK_LLMS_LIST)} mock LLMs.")])
    except sqlite3.Error as e:
        color_print([("red", f"Database error during model population: {e}")])
    finally:
        if conn:
            conn.close()

def create_mock_llm_file():
    """Creates the mock_llms.py file in the designated samples directory."""
    if not LLM_SAMPLES_DIR.exists():
        LLM_SAMPLES_DIR.mkdir(parents=True, exist_ok=True)
    
    with open(MOCK_LLM_FILE, "w") as f:
        f.write(MOCK_LLM_CONTENT)
    color_print([("green", f"Successfully created mock LLM definition file: {MOCK_LLM_FILE}")])

def destroy_mock_llm_data():
    """Destroys the mock LLM data file and database."""
    print(f"Attempting to destroy mock LLM data in: {SAMPLES_ROOT}")
    
    # Remove the mock_llms.py file
    if MOCK_LLM_FILE.exists():
        try:
            os.remove(MOCK_LLM_FILE)
            color_print([("green", f"Successfully destroyed mock LLM file: {MOCK_LLM_FILE}")])
        except OSError as e:
            color_print([("red", f"Error destroying mock LLM file: {e}")])
    else:
        print(f"Skipping destruction: {MOCK_LLM_FILE} does not exist.")

    # Remove the database file
    if DATABASE_FILE.exists():
        try:
            os.remove(DATABASE_FILE)
            color_print([("green", f"Successfully destroyed database file: {DATABASE_FILE}")])
        except OSError as e:
            color_print([("red", f"Error destroying database file: {e}")])
    else:
        print(f"Skipping destruction: {DATABASE_FILE} does not exist.")

    if not MOCK_LLM_FILE.exists() and not DATABASE_FILE.exists():
        print("No mock LLM data files or database found. Nothing to destroy.")
    else:
        print("Mock LLM data destruction process complete.")


def run_command(command: str, cwd: Path = PROJECT_ROOT) -> int:
    try:
        process = subprocess.Popen(
            command,
            shell=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            cwd=cwd,
            text=True,
            encoding="utf-8",
        )
        for line in iter(process.stdout.readline, ""):
            print(line, end="")
        process.stdout.close()
        return process.wait()
    except Exception as e:
        color_print([("red", f"An error occurred: {e}")])
        return 1

def print_header(title: str):
    print("\n" + "=" * 70)
    print(f"{title}")
    print("=" * 70)

def main():
    print_header("JennAI Data Samples Admin Console")

    MENU = [
        Choice("install_data", "Install Mock LLM Data and Database (Create)"),
        Choice("destroy_data", "Destroy Mock LLM Data and Database (Destroy)"),
        Separator(),
        Choice("exit", "Exit"),
    ]

    while True:
        try:
            selection = inquirer.select(
                message="Select a data sample task:",
                choices=MENU,
                default="install_data",
                qmark=">",
                cycle=False,
                max_height=10,
            ).execute()

            if selection == "install_data":
                print_header("Installing Mock LLM Data")
                destroy_mock_llm_data() # Ensure clean slate before creating
                initialize_database()
                populate_mock_models()
                create_mock_llm_file()
                color_print([("green", "\nMock LLM data installation complete.")])
                input("\nPress Enter to return to the menu...")

            elif selection == "destroy_data":
                print_header("Destroying Mock LLM Data")
                destroy_mock_llm_data()
                color_print([("green", "\nMock LLM data destruction complete.")])
                input("\nPress Enter to return to the menu...")

            elif selection == "exit":
                print("\nGoodbye!")
                break

        except KeyboardInterrupt:
            color_print([("red", "\nOperation cancelled by user.")])
            break

if __name__ == "__main__":
    main()